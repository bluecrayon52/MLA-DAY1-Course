{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\".././images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "# MLU Day One Machine Learning - Walkthrough & Advanced AutoGluon Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Walkthrough & Discussions\n",
    "Now that you have finished your hands-on activity, let's walk through the code you have used and discuss it. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20210814_182843/\"\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20210814_182843/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    5051\n",
      "Train Data Columns: 9\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (4.149249912590282, 1.414973347970818, 2.60147, 0.33003)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5203.6 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.91 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Title', 'Edition', 'Ratings', 'Synopsis']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 4920\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])          : 1 | ['ID']\n",
      "\t\t('object', [])       : 4 | ['Author', 'Reviews', 'Genre', 'BookCategory']\n",
      "\t\t('object', ['text']) : 4 | ['Title', 'Edition', 'Ratings', 'Synopsis']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :    4 | ['Author', 'Reviews', 'Genre', 'BookCategory']\n",
      "\t\t('category', ['text_as_category'])  :    4 | ['Title', 'Edition', 'Ratings', 'Synopsis']\n",
      "\t\t('int', [])                         :    1 | ['ID']\n",
      "\t\t('int', ['binned', 'text_special']) :   81 | ['Title.char_count', 'Title.word_count', 'Title.capital_ratio', 'Title.lower_ratio', 'Title.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 4921 | ['__nlp__.000', '__nlp__.10', '__nlp__.10 customer', '__nlp__.10 customer reviews', '__nlp__.100', ...]\n",
      "\t35.0s = Fit runtime\n",
      "\t9 features in original data used to generate 5011 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 25.52 MB (0.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 35.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 4545, Val Rows: 506\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 24.31s of the 24.2s of remaining time.\n",
      "\t-0.3662\t = Validation root_mean_squared_error score\n",
      "\t0.8s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 23.39s of the 23.28s of remaining time.\n",
      "\t-0.3771\t = Validation root_mean_squared_error score\n",
      "\t0.81s\t = Training runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 22.46s of the 22.35s of remaining time.\n",
      "\t-0.2157\t = Validation root_mean_squared_error score\n",
      "\t9.37s\t = Training runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 12.94s of the 12.83s of remaining time.\n",
      "\t-0.2139\t = Validation root_mean_squared_error score\n",
      "\t8.52s\t = Training runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 4.26s of the 4.16s of remaining time.\n",
      "\t-0.2358\t = Validation root_mean_squared_error score\n",
      "\t347.21s\t = Training runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 24.3s of the -344.49s of remaining time.\n",
      "\t-0.2118\t = Validation root_mean_squared_error score\n",
      "\t0.53s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 405.42s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20210814_182843/\")\n"
     ]
    }
   ],
   "source": [
    "# Importing the newly installed AutoGluon code library\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset\n",
    "\n",
    "train = TabularDataset(\".././datasets/training.csv\")\n",
    "mlu_test_data = TabularDataset(\".././datasets/mlu-leaderboard-test.csv\")\n",
    "\n",
    "\n",
    "predictor = TabularPredictor(label=\"Price\").fit(train_data=train, time_limit=60)\n",
    "\n",
    "predictions = predictor.predict(mlu_test_data)\n",
    "\n",
    "# Creating a new dataframe for the submission\n",
    "submission = mlu_test_data[[\"ID\"]].copy(deep=True)\n",
    "\n",
    "# Creating label column from price prediction list\n",
    "submission[\"Price\"] = predictions\n",
    "\n",
    "# Saving our csv file for Leaderboard submission\n",
    "# index=False prevents printing the row IDs as separate values\n",
    "submission.to_csv(\n",
    "    \".././datasets/predictions/Solution-Demo.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part II - Advanced AutoGluon Features\n",
    "\n",
    "## ML Problem Description\n",
    "Predict the occupation of individuals using census data. \n",
    "> This is a multiclass classification task (15 distinct classes). <br>\n",
    "\n",
    "For the advanced feature demonstration we want to use a new dataset: Census data. In this particular dataset, each row corresponds to an individual person, and the columns contain various demographic characteristics collected for the census.\n",
    "\n",
    "We’ll predict the occupation of an individual - this is a multiclass classification problem. Start by importing AutoGluon’s `TabularPredictor` and `TabularDataset`, and load the data from a S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q bokeh==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv | Columns = 15 / 15 | Rows = 39073 -> 39073\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6118</th>\n",
       "      <td>51</td>\n",
       "      <td>Private</td>\n",
       "      <td>39264</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23204</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>51662</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29590</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>326310</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18116</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>222450</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "      <td>40</td>\n",
       "      <td>El-Salvador</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33964</th>\n",
       "      <td>62</td>\n",
       "      <td>Private</td>\n",
       "      <td>109190</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age workclass  fnlwgt      education  education-num  \\\n",
       "6118    51   Private   39264   Some-college             10   \n",
       "23204   58   Private   51662           10th              6   \n",
       "29590   40   Private  326310   Some-college             10   \n",
       "18116   37   Private  222450        HS-grad              9   \n",
       "33964   62   Private  109190      Bachelors             13   \n",
       "\n",
       "            marital-status        occupation    relationship    race      sex  \\\n",
       "6118    Married-civ-spouse   Exec-managerial            Wife   White   Female   \n",
       "23204   Married-civ-spouse     Other-service            Wife   White   Female   \n",
       "29590   Married-civ-spouse      Craft-repair         Husband   White     Male   \n",
       "18116        Never-married             Sales   Not-in-family   White     Male   \n",
       "33964   Married-civ-spouse   Exec-managerial         Husband   White     Male   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week  native-country   class  \n",
       "6118              0             0              40   United-States    >50K  \n",
       "23204             0             0               8   United-States   <=50K  \n",
       "29590             0             0              44   United-States   <=50K  \n",
       "18116             0          2339              40     El-Salvador   <=50K  \n",
       "33964         15024             0              40   United-States    >50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load in the dataset\n",
    "train_data = TabularDataset(\"https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv\")\n",
    "# Subsample a subset of data for faster demo, try setting this to much larger values\n",
    "subsample_size = 5000\n",
    "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of occupation column: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count              5000\n",
       "unique               15\n",
       "top        Craft-repair\n",
       "freq                672\n",
       "Name: occupation, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign column that contains the label to a variable that can be re-used later\n",
    "label = \"occupation\"\n",
    "\n",
    "print(\"Summary of occupation column: \\n\")\n",
    "train_data[\"occupation\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, validation, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv | Columns = 15 / 15 | Rows = 9769 -> 9769\n"
     ]
    }
   ],
   "source": [
    "# Create a train & validation split\n",
    "train_data, val_data = train_test_split(\n",
    "    train_data, test_size=0.1, shuffle=True, random_state=23\n",
    ")\n",
    "\n",
    "# Let's load the test data\n",
    "test_data = TabularDataset(\"https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv\")\n",
    "\n",
    "# We need to split the test dataset into a features and a label subset\n",
    "y_test = test_data[label]\n",
    "test_data_nolabel = test_data.drop(columns=[label])  # delete label column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify eval-metric just for demo (unnecessary as it's the default)\n",
    "metric = \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full list of parameters can be found here:\n",
    "\n",
    "`'accuracy', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_weighted', 'roc_auc', 'average_precision', 'precision', 'precision_macro', 'precision_micro', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_weighted', 'log_loss', 'pac_score'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying hyperparameters and tuning them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogluon.core as ag\n",
    "\n",
    "# Set Neural Net options\n",
    "# Specifies non-default hyperparameter values for neural network models\n",
    "nn_options = {\n",
    "    # number of training epochs (controls training time of NN models)\n",
    "    \"num_epochs\": 10,\n",
    "    # learning rate used in training (real-valued hyperparameter searched on log-scale)\n",
    "    \"learning_rate\": ag.space.Real(1e-4, 1e-2, default=5e-4, log=True),\n",
    "    # activation function used in NN (categorical hyperparameter, default = first entry)\n",
    "    \"activation\": ag.space.Categorical(\"relu\", \"softrelu\", \"tanh\"),\n",
    "    # each choice for categorical hyperparameter 'layers' corresponds to list of sizes for each NN layer to use\n",
    "    \"layers\": ag.space.Categorical([100], [1000], [200, 100], [300, 200, 100]),\n",
    "    # dropout probability (real-valued hyperparameter)\n",
    "    \"dropout_prob\": ag.space.Real(0.0, 0.5, default=0.1),\n",
    "}\n",
    "\n",
    "# Set GBM options\n",
    "# Specifies non-default hyperparameter values for lightGBM gradient boosted trees\n",
    "gbm_options = {\n",
    "    # number of boosting rounds (controls training time of GBM models)\n",
    "    \"num_boost_round\": 100,\n",
    "    # number of leaves in trees (integer hyperparameter)\n",
    "    \"num_leaves\": ag.space.Int(lower=26, upper=66, default=36),\n",
    "}\n",
    "\n",
    "# Add both NN and GBM options into a hyperparameter dictionary\n",
    "# hyperparameters of each model type\n",
    "# When these keys are missing from the hyperparameters dict, no models of that type are trained\n",
    "hyperparameters = {\n",
    "    \"GBM\": gbm_options,\n",
    "    \"NN\": nn_options,\n",
    "}\n",
    "\n",
    "# Train various models for ~2 min\n",
    "time_limit = 2 * 60\n",
    "# Number of trials for hyperparameters\n",
    "num_trials = 5\n",
    "\n",
    "# To tune hyperparameters using Bayesian optimization to find best combination of params\n",
    "search_strategy = \"auto\"\n",
    "\n",
    "# HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "hyperparameter_tune_kwargs = {\n",
    "    \"num_trials\": num_trials,\n",
    "    \"scheduler\": \"local\",\n",
    "    \"searcher\": search_strategy,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying settings for TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train various models for ~2 min\n",
    "time_limit = 2 * 60\n",
    "# Number of trials for hyperparameters\n",
    "num_trials = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model using TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20210814_183542/\"\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Beginning AutoGluon training ... Time limit = 120s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20210814_183542/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    4500\n",
      "Train Data Columns: 14\n",
      "Tuning Data Rows:    500\n",
      "Tuning Data Columns: 14\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Sales', ' Adm-clerical', ' ?', ' Prof-specialty', ' Other-service', ' Machine-op-inspct', ' Craft-repair', ' Exec-managerial', ' Handlers-cleaners', ' Transport-moving']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 14 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9995555555555555\n",
      "Train Data Class Count: 14\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14259.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.89 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])      : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.28 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Hyperparameter tuning model: LightGBM ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a11f4fe066e47139ad1bc3f78e7aee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 16. Best iteration is:\n",
      "\t[6]\ttrain_set's multi_error: 0.5249\tvalid_set's multi_error: 0.599198\n",
      "\tTime limit exceeded\n",
      "Fitted model: LightGBM/T0 ...\n",
      "\t0.4008\t = Validation accuracy score\n",
      "\t53.93s\t = Training runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetMXNet ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae90900ddae478da904503de61b3471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitted model: NeuralNetMXNet/T0 ...\n",
      "\t0.4028\t = Validation accuracy score\n",
      "\t7.35s\t = Training runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitted model: NeuralNetMXNet/T1 ...\n",
      "\t0.3888\t = Validation accuracy score\n",
      "\t6.6s\t = Training runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitted model: NeuralNetMXNet/T2 ...\n",
      "\t0.4008\t = Validation accuracy score\n",
      "\t9.76s\t = Training runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitted model: NeuralNetMXNet/T3 ...\n",
      "\t0.4008\t = Validation accuracy score\n",
      "\t7.2s\t = Training runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitted model: NeuralNetMXNet/T4 ...\n",
      "\t0.2906\t = Validation accuracy score\n",
      "\t7.19s\t = Training runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.83s of the 21.76s of remaining time.\n",
      "\t0.4128\t = Validation accuracy score\n",
      "\t0.34s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 98.6s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20210814_183542/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label, eval_metric=metric).fit(\n",
    "    train_data,\n",
    "    tuning_data=val_data,\n",
    "    time_limit=time_limit,\n",
    "    hyperparameters=hyperparameters,\n",
    "    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [' Other-service', ' Craft-repair', ' Craft-repair', ' Other-service', ' Other-service']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: accuracy on test data: 0.37015047599549594\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.37015047599549594\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictor.predict(test_data_nolabel)\n",
    "print(f\"Predictions:  {list(y_pred)[:5]}\")\n",
    "perf = predictor.evaluate(test_data, auxiliary_metrics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following to view a summary of what happened during the fit. Now this command will show details of the hyperparameter-tuning process for each type of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2   0.412826       0.141045  71.390454                0.000708           0.340977            2       True          7\n",
      "1    NeuralNetMXNet/T0   0.402806       0.053254   7.354919                0.053254           7.354919            1       True          2\n",
      "2          LightGBM/T0   0.400802       0.027492  53.932863                0.027492          53.932863            1       True          1\n",
      "3    NeuralNetMXNet/T3   0.400802       0.053175   7.196842                0.053175           7.196842            1       True          5\n",
      "4    NeuralNetMXNet/T2   0.400802       0.059591   9.761696                0.059591           9.761696            1       True          4\n",
      "5    NeuralNetMXNet/T1   0.388778       0.052324   6.600852                0.052324           6.600852            1       True          3\n",
      "6    NeuralNetMXNet/T4   0.290581       0.053222   7.189631                0.053222           7.189631            1       True          6\n",
      "Number of models trained: 7\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'LGBModel', 'TabularNeuralNetModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "('int', [])      : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20210814_183542/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'LightGBM/T0': 'LGBModel',\n",
       "  'NeuralNetMXNet/T0': 'TabularNeuralNetModel',\n",
       "  'NeuralNetMXNet/T1': 'TabularNeuralNetModel',\n",
       "  'NeuralNetMXNet/T2': 'TabularNeuralNetModel',\n",
       "  'NeuralNetMXNet/T3': 'TabularNeuralNetModel',\n",
       "  'NeuralNetMXNet/T4': 'TabularNeuralNetModel',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'LightGBM/T0': 0.40080160320641284,\n",
       "  'NeuralNetMXNet/T0': 0.4028056112224449,\n",
       "  'NeuralNetMXNet/T1': 0.38877755511022044,\n",
       "  'NeuralNetMXNet/T2': 0.40080160320641284,\n",
       "  'NeuralNetMXNet/T3': 0.40080160320641284,\n",
       "  'NeuralNetMXNet/T4': 0.2905811623246493,\n",
       "  'WeightedEnsemble_L2': 0.41282565130260523},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'LightGBM/T0': 'AutogluonModels/ag-20210814_183542/models/LightGBM/T0/',\n",
       "  'NeuralNetMXNet/T0': 'AutogluonModels/ag-20210814_183542/models/NeuralNetMXNet/T0/',\n",
       "  'NeuralNetMXNet/T1': 'AutogluonModels/ag-20210814_183542/models/NeuralNetMXNet/T1/',\n",
       "  'NeuralNetMXNet/T2': 'AutogluonModels/ag-20210814_183542/models/NeuralNetMXNet/T2/',\n",
       "  'NeuralNetMXNet/T3': 'AutogluonModels/ag-20210814_183542/models/NeuralNetMXNet/T3/',\n",
       "  'NeuralNetMXNet/T4': 'AutogluonModels/ag-20210814_183542/models/NeuralNetMXNet/T4/',\n",
       "  'WeightedEnsemble_L2': 'AutogluonModels/ag-20210814_183542/models/WeightedEnsemble_L2/'},\n",
       " 'model_fit_times': {'LightGBM/T0': 53.932862520217896,\n",
       "  'NeuralNetMXNet/T0': 7.354918956756592,\n",
       "  'NeuralNetMXNet/T1': 6.600852012634277,\n",
       "  'NeuralNetMXNet/T2': 9.761695623397827,\n",
       "  'NeuralNetMXNet/T3': 7.196842432022095,\n",
       "  'NeuralNetMXNet/T4': 7.189631223678589,\n",
       "  'WeightedEnsemble_L2': 0.3409767150878906},\n",
       " 'model_pred_times': {'LightGBM/T0': 0.02749180793762207,\n",
       "  'NeuralNetMXNet/T0': 0.053254127502441406,\n",
       "  'NeuralNetMXNet/T1': 0.05232429504394531,\n",
       "  'NeuralNetMXNet/T2': 0.059591054916381836,\n",
       "  'NeuralNetMXNet/T3': 0.05317521095275879,\n",
       "  'NeuralNetMXNet/T4': 0.0532221794128418,\n",
       "  'WeightedEnsemble_L2': 0.0007078647613525391},\n",
       " 'num_bag_folds': 0,\n",
       " 'max_stack_level': 2,\n",
       " 'num_classes': 14,\n",
       " 'model_hyperparams': {'LightGBM/T0': {'num_boost_round': 100,\n",
       "   'num_threads': 4,\n",
       "   'learning_rate': 0.05,\n",
       "   'objective': 'multiclass',\n",
       "   'num_classes': 14,\n",
       "   'verbose': -1,\n",
       "   'boosting_type': 'gbdt',\n",
       "   'two_round': True,\n",
       "   'num_leaves': 36,\n",
       "   'feature_fraction': 1.0,\n",
       "   'min_data_in_leaf': 20,\n",
       "   'seed_value': None},\n",
       "  'NeuralNetMXNet/T0': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'seed_value': None,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'network_type': 'widedeep',\n",
       "   'layers': [100],\n",
       "   'numeric_embed_dim': None,\n",
       "   'activation': 'relu',\n",
       "   'max_layer_width': 2056,\n",
       "   'embedding_size_factor': 1.0,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'use_batchnorm': True,\n",
       "   'dropout_prob': 0.1,\n",
       "   'batch_size': 512,\n",
       "   'loss_function': None,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0005,\n",
       "   'weight_decay': 1e-06,\n",
       "   'clip_gradient': 100.0,\n",
       "   'momentum': 0.9,\n",
       "   'lr_scheduler': None,\n",
       "   'base_lr': 3e-05,\n",
       "   'target_lr': 1.0,\n",
       "   'lr_decay': 0.1,\n",
       "   'warmup_epochs': 10,\n",
       "   'use_ngram_features': False},\n",
       "  'NeuralNetMXNet/T1': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'seed_value': None,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'network_type': 'widedeep',\n",
       "   'layers': [100],\n",
       "   'numeric_embed_dim': None,\n",
       "   'activation': 'relu',\n",
       "   'max_layer_width': 2056,\n",
       "   'embedding_size_factor': 0.6995606726235626,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'use_batchnorm': False,\n",
       "   'dropout_prob': 0.4557032083075756,\n",
       "   'batch_size': 512,\n",
       "   'loss_function': None,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.007168276421102162,\n",
       "   'weight_decay': 7.571352382969593e-06,\n",
       "   'clip_gradient': 100.0,\n",
       "   'momentum': 0.9,\n",
       "   'lr_scheduler': None,\n",
       "   'base_lr': 3e-05,\n",
       "   'target_lr': 1.0,\n",
       "   'lr_decay': 0.1,\n",
       "   'warmup_epochs': 10,\n",
       "   'use_ngram_features': False},\n",
       "  'NeuralNetMXNet/T2': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'seed_value': None,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'network_type': 'widedeep',\n",
       "   'layers': [300, 200, 100],\n",
       "   'numeric_embed_dim': None,\n",
       "   'activation': 'relu',\n",
       "   'max_layer_width': 2056,\n",
       "   'embedding_size_factor': 1.276452840052062,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'use_batchnorm': True,\n",
       "   'dropout_prob': 0.3380231599804919,\n",
       "   'batch_size': 512,\n",
       "   'loss_function': None,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0080708800023561,\n",
       "   'weight_decay': 3.3959319952029775e-09,\n",
       "   'clip_gradient': 100.0,\n",
       "   'momentum': 0.9,\n",
       "   'lr_scheduler': None,\n",
       "   'base_lr': 3e-05,\n",
       "   'target_lr': 1.0,\n",
       "   'lr_decay': 0.1,\n",
       "   'warmup_epochs': 10,\n",
       "   'use_ngram_features': False},\n",
       "  'NeuralNetMXNet/T3': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'seed_value': None,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'network_type': 'feedforward',\n",
       "   'layers': [100],\n",
       "   'numeric_embed_dim': None,\n",
       "   'activation': 'softrelu',\n",
       "   'max_layer_width': 2056,\n",
       "   'embedding_size_factor': 0.9920468262029063,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'use_batchnorm': True,\n",
       "   'dropout_prob': 0.48659433757911963,\n",
       "   'batch_size': 512,\n",
       "   'loss_function': None,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.005998257947355948,\n",
       "   'weight_decay': 5.947608218116768e-05,\n",
       "   'clip_gradient': 100.0,\n",
       "   'momentum': 0.9,\n",
       "   'lr_scheduler': None,\n",
       "   'base_lr': 3e-05,\n",
       "   'target_lr': 1.0,\n",
       "   'lr_decay': 0.1,\n",
       "   'warmup_epochs': 10,\n",
       "   'use_ngram_features': False},\n",
       "  'NeuralNetMXNet/T4': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'seed_value': None,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'network_type': 'feedforward',\n",
       "   'layers': [200, 100],\n",
       "   'numeric_embed_dim': None,\n",
       "   'activation': 'softrelu',\n",
       "   'max_layer_width': 2056,\n",
       "   'embedding_size_factor': 0.6312603685298656,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'use_batchnorm': False,\n",
       "   'dropout_prob': 0.40255479899430474,\n",
       "   'batch_size': 512,\n",
       "   'loss_function': None,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0022719572135935654,\n",
       "   'weight_decay': 7.412515114051063e-11,\n",
       "   'clip_gradient': 100.0,\n",
       "   'momentum': 0.9,\n",
       "   'lr_scheduler': None,\n",
       "   'base_lr': 3e-05,\n",
       "   'target_lr': 1.0,\n",
       "   'lr_decay': 0.1,\n",
       "   'warmup_epochs': 10,\n",
       "   'use_ngram_features': False},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                  model  score_val  pred_time_val   fit_time  \\\n",
       " 0  WeightedEnsemble_L2   0.412826       0.141045  71.390454   \n",
       " 1    NeuralNetMXNet/T0   0.402806       0.053254   7.354919   \n",
       " 2          LightGBM/T0   0.400802       0.027492  53.932863   \n",
       " 3    NeuralNetMXNet/T3   0.400802       0.053175   7.196842   \n",
       " 4    NeuralNetMXNet/T2   0.400802       0.059591   9.761696   \n",
       " 5    NeuralNetMXNet/T1   0.388778       0.052324   6.600852   \n",
       " 6    NeuralNetMXNet/T4   0.290581       0.053222   7.189631   \n",
       " \n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                0.000708           0.340977            2       True   \n",
       " 1                0.053254           7.354919            1       True   \n",
       " 2                0.027492          53.932863            1       True   \n",
       " 3                0.053175           7.196842            1       True   \n",
       " 4                0.059591           9.761696            1       True   \n",
       " 5                0.052324           6.600852            1       True   \n",
       " 6                0.053222           7.189631            1       True   \n",
       " \n",
       "    fit_order  \n",
       " 0          7  \n",
       " 1          2  \n",
       " 2          1  \n",
       " 3          5  \n",
       " 4          4  \n",
       " 5          3  \n",
       " 6          6  }"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the predictive performance may be poor because we are using few training datapoints and small ranges for hyperparameters to ensure quick runtimes. You can call `fit()` multiple times while modifying these settings to better understand how these choices affect performance outcomes. For example: you can increase `subsample_size` to train using a larger dataset, increase the `num_epochs` and `num_boost_round` hyperparameters, and increase the `time_limit` (which you should do for all code in these tutorials). To see more detailed output during the execution of `fit()`, you can also pass in the argument: `verbosity = 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model ensembling with stacking/bagging\n",
    "Beyond hyperparameter-tuning with a correctly-specified evaluation metric, thera re two other methods to boost predictive performance:\n",
    "- bagging and \n",
    "- stack-ensembling\n",
    "\n",
    "You’ll often see performance improve if you specify `num_bag_folds = 5-10`, `num_stack_levels = 1-3` in the call to `fit()`. Beware that doing this will increase training times and memory/disk usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20210814_183733/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20210814_183733/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    4500\n",
      "Train Data Columns: 14\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Sales', ' Adm-clerical', ' ?', ' Prof-specialty', ' Other-service', ' Machine-op-inspct', ' Craft-repair', ' Exec-managerial', ' Handlers-cleaners', ' Transport-moving']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 14 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9995555555555555\n",
      "Train Data Class Count: 14\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14283.6 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.6 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])      : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.25 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\t0.3748\t = Validation accuracy score\n",
      "\t11.11s\t = Training runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L1 ...\n",
      "\t0.1621\t = Validation accuracy score\n",
      "\t13.86s\t = Training runtime\n",
      "\t1.94s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.3762\t = Validation accuracy score\n",
      "\t0.76s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\t0.3637\t = Validation accuracy score\n",
      "\t14.98s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L2 ...\n",
      "\t0.2817\t = Validation accuracy score\n",
      "\t15.22s\t = Training runtime\n",
      "\t2.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t0.3655\t = Validation accuracy score\n",
      "\t0.76s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 61.36s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20210814_183733/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label, eval_metric=metric).fit(\n",
    "    train_data,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=1,\n",
    "    num_stack_levels=1,\n",
    "    # last  argument is just for quick demo here, omit it in real applications\n",
    "    hyperparameters={\n",
    "        \"NN\": {\"num_epochs\": 2},\n",
    "        \"GBM\": {\"num_boost_round\": 20},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should not provide `tuning_data` when stacking/bagging, and instead provide all your available data as train_data (which AutoGluon will split in more intelligent ways). Parameter `num_bag_sets` controls how many times the K-fold bagging process is repeated to further reduce variance (increasing this may further boost accuracy but will substantially increase training times, inference latency, and memory/disk usage). Rather than manually searching for good bagging/stacking values yourself, AutoGluon will automatically select good values for you if you specify `auto_stack` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"agModels-predictOccupation/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    4500\n",
      "Train Data Columns: 14\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Sales', ' Adm-clerical', ' ?', ' Prof-specialty', ' Other-service', ' Machine-op-inspct', ' Craft-repair', ' Exec-managerial', ' Handlers-cleaners', ' Transport-moving']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 14 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9995555555555555\n",
      "Train Data Class Count: 14\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14080.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.6 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])      : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.25 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.2s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 19.86s of the 29.8s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 14. Best iteration is:\n",
      "\t[14]\ttrain_set's multi_error: 0.48246\tvalid_set's multi_error: 0.631111\n",
      "\tRan out of time, early stopping on iteration 15. Best iteration is:\n",
      "\t[11]\ttrain_set's multi_error: 0.491601\tvalid_set's multi_error: 0.611111\n",
      "\tRan out of time, early stopping on iteration 15. Best iteration is:\n",
      "\t[9]\ttrain_set's multi_error: 0.50914\tvalid_set's multi_error: 0.6\n",
      "\tRan out of time, early stopping on iteration 14. Best iteration is:\n",
      "\t[8]\ttrain_set's multi_error: 0.521986\tvalid_set's multi_error: 0.617778\n",
      "\tRan out of time, early stopping on iteration 16. Best iteration is:\n",
      "\t[15]\ttrain_set's multi_error: 0.487401\tvalid_set's multi_error: 0.644444\n",
      "\tRan out of time, early stopping on iteration 16. Best iteration is:\n",
      "\t[14]\ttrain_set's multi_error: 0.477767\tvalid_set's multi_error: 0.62\n",
      "\tRan out of time, early stopping on iteration 17. Best iteration is:\n",
      "\t[5]\ttrain_set's multi_error: 0.543231\tvalid_set's multi_error: 0.604444\n",
      "\tRan out of time, early stopping on iteration 18. Best iteration is:\n",
      "\t[7]\ttrain_set's multi_error: 0.527421\tvalid_set's multi_error: 0.628889\n",
      "\tRan out of time, early stopping on iteration 18. Best iteration is:\n",
      "\t[5]\ttrain_set's multi_error: 0.542356\tvalid_set's multi_error: 0.625835\n",
      "\t0.3828\t = Validation accuracy score\n",
      "\t18.83s\t = Training runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L1 ... Training model for up to 0.64s of the 10.57s of remaining time.\n",
      "\tTime limit exceeded... Skipping NeuralNetMXNet_BAG_L1.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.8s of the 9.99s of remaining time.\n",
      "\t0.3828\t = Validation accuracy score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\ttrain_set's multi_error: 0.598567\tvalid_set's multi_error: 0.706667\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "Fitting model: NeuralNetMXNet_BAG_L2 ... Training model for up to 5.83s of the 5.83s of remaining time.\n",
      "\tTime limit exceeded... Skipping NeuralNetMXNet_BAG_L2.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "No base models to train on, skipping weighted ensemble...\n",
      "AutoGluon training complete, total runtime = 24.89s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictOccupation/\")\n"
     ]
    }
   ],
   "source": [
    "# Folder where to store trained models\n",
    "save_path = \"agModels-predictOccupation\"\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric=metric, path=save_path).fit(\n",
    "    train_data,\n",
    "    auto_stack=True,\n",
    "    time_limit=30,\n",
    "    # Last 2 arguments are for quick demo, omit them in real applications\n",
    "    hyperparameters={\n",
    "        \"NN\": {\"num_epochs\": 2},\n",
    "        \"GBM\": {\"num_boost_round\": 20},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often stacking/bagging will produce superior accuracy than hyperparameter-tuning, but you may try combining both techniques (note: specifying `presets='best_quality'` in `fit()` simply sets `auto_stack=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction options (inference)\n",
    "\n",
    "Even if you’ve started a new Python session since last calling `fit()`, you can still load a previously trained predictor from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `predictor.path` is another way to get the relative path needed to later load predictor.\n",
    "predictor = TabularPredictor.load(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above `save_path` is the same folder previously passed to `TabularPredictor`, in which all the trained models have been saved. You can train easily models on one machine and deploy them on another. Simply copy the `save_path` folder to the new machine and specify its new path in `TabularPredictor.load()`.\n",
    "\n",
    "We can make a prediction on an individual example rather than on a full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Other-service\n",
       "Name: occupation, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: .iloc[0] won't work because it returns pandas Series instead of DataFrame\n",
    "datapoint = test_data_nolabel.iloc[[0]]\n",
    "\n",
    "predictor.predict(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To output predicted class probabilities instead of predicted classes, you can use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>?</th>\n",
       "      <th>Adm-clerical</th>\n",
       "      <th>Armed-Forces</th>\n",
       "      <th>Craft-repair</th>\n",
       "      <th>Exec-managerial</th>\n",
       "      <th>Farming-fishing</th>\n",
       "      <th>Handlers-cleaners</th>\n",
       "      <th>Machine-op-inspct</th>\n",
       "      <th>Other-service</th>\n",
       "      <th>Priv-house-serv</th>\n",
       "      <th>Prof-specialty</th>\n",
       "      <th>Protective-serv</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Tech-support</th>\n",
       "      <th>Transport-moving</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038169</td>\n",
       "      <td>0.106904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090346</td>\n",
       "      <td>0.089637</td>\n",
       "      <td>0.021648</td>\n",
       "      <td>0.052957</td>\n",
       "      <td>0.054275</td>\n",
       "      <td>0.278535</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.083228</td>\n",
       "      <td>0.015408</td>\n",
       "      <td>0.111708</td>\n",
       "      <td>0.019857</td>\n",
       "      <td>0.033182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ?   Adm-clerical   Armed-Forces   Craft-repair   Exec-managerial  \\\n",
       "0  0.038169       0.106904            0.0       0.090346          0.089637   \n",
       "\n",
       "    Farming-fishing   Handlers-cleaners   Machine-op-inspct   Other-service  \\\n",
       "0          0.021648            0.052957            0.054275        0.278535   \n",
       "\n",
       "    Priv-house-serv   Prof-specialty   Protective-serv     Sales  \\\n",
       "0          0.004144         0.083228          0.015408  0.111708   \n",
       "\n",
       "    Tech-support   Transport-moving  \n",
       "0       0.019857           0.033182  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a DataFrame that shows which probability corresponds to which class\n",
    "predictor.predict_proba(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `predict()` and `predict_proba()` will utilize the model that AutoGluon thinks is most accurate, which is usually an ensemble of many individual models. Here’s how to see which model this corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LightGBM_BAG_L1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.get_model_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instead specify a particular model to use for predictions (e.g. to reduce inference latency). Note that a ‘model’ in AutoGluon may refer to for example a single Neural Network, a bagged ensemble of many Neural Network copies trained on different training/validation splits, a weighted ensemble that aggregates the predictions of many other models, or a stacked model that operates on predictions output by other models. This is akin to viewing a RandomForest as one ‘model’ when it is in fact an ensemble of many decision trees.\n",
    "\n",
    "Before deciding which model to use, let’s evaluate all of the models AutoGluon has previously trained on our test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGluon leaderboard function options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.369843</td>\n",
       "      <td>0.382837</td>\n",
       "      <td>1.433240</td>\n",
       "      <td>0.243411</td>\n",
       "      <td>18.828165</td>\n",
       "      <td>1.433240</td>\n",
       "      <td>0.243411</td>\n",
       "      <td>18.828165</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.369843</td>\n",
       "      <td>0.382837</td>\n",
       "      <td>1.437013</td>\n",
       "      <td>0.244729</td>\n",
       "      <td>18.831213</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  score_val  pred_time_test  pred_time_val  \\\n",
       "0      LightGBM_BAG_L1    0.369843   0.382837        1.433240       0.243411   \n",
       "1  WeightedEnsemble_L2    0.369843   0.382837        1.437013       0.244729   \n",
       "\n",
       "    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0  18.828165                 1.433240                0.243411   \n",
       "1  18.831213                 0.003772                0.001318   \n",
       "\n",
       "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0          18.828165            1       True          1  \n",
       "1           0.003048            2       True          2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaderboard shows each model’s predictive performance on the test data (`score_test`) and validation data (`score_val`), as well as the time required to: produce predictions for the test data (`pred_time_val`), produce predictions on the validation data (`pred_time_val`), and train only this model (`fit_time`). Below, we show that a leaderboard can be produced without new data (just uses the data previously reserved for validation inside `fit`) and can display extra information about each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "      <th>num_features</th>\n",
       "      <th>...</th>\n",
       "      <th>child_model_type</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>hyperparameters_fit</th>\n",
       "      <th>ag_args_fit</th>\n",
       "      <th>features</th>\n",
       "      <th>child_hyperparameters</th>\n",
       "      <th>child_hyperparameters_fit</th>\n",
       "      <th>child_ag_args_fit</th>\n",
       "      <th>ancestors</th>\n",
       "      <th>descendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.382837</td>\n",
       "      <td>0.243411</td>\n",
       "      <td>18.828165</td>\n",
       "      <td>0.243411</td>\n",
       "      <td>18.828165</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>LGBModel</td>\n",
       "      <td>{'use_orig_features': True, 'max_base_models':...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[education-num, age, fnlwgt, hours-per-week, e...</td>\n",
       "      <td>{'num_boost_round': 20, 'num_threads': -1, 'le...</td>\n",
       "      <td>{'num_boost_round': 11}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.382837</td>\n",
       "      <td>0.244729</td>\n",
       "      <td>18.831213</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>GreedyWeightedEnsembleModel</td>\n",
       "      <td>{'use_orig_features': False, 'max_base_models'...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[LightGBM_BAG_L1_9, LightGBM_BAG_L1_11, LightG...</td>\n",
       "      <td>{'ensemble_size': 100}</td>\n",
       "      <td>{'ensemble_size': 1}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[LightGBM_BAG_L1]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_val  pred_time_val   fit_time  \\\n",
       "0      LightGBM_BAG_L1   0.382837       0.243411  18.828165   \n",
       "1  WeightedEnsemble_L2   0.382837       0.244729  18.831213   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.243411          18.828165            1       True   \n",
       "1                0.001318           0.003048            2       True   \n",
       "\n",
       "   fit_order  num_features  ...             child_model_type  \\\n",
       "0          1            14  ...                     LGBModel   \n",
       "1          2            14  ...  GreedyWeightedEnsembleModel   \n",
       "\n",
       "                                     hyperparameters  hyperparameters_fit  \\\n",
       "0  {'use_orig_features': True, 'max_base_models':...                   {}   \n",
       "1  {'use_orig_features': False, 'max_base_models'...                   {}   \n",
       "\n",
       "                                         ag_args_fit  \\\n",
       "0  {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "1  {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "\n",
       "                                            features  \\\n",
       "0  [education-num, age, fnlwgt, hours-per-week, e...   \n",
       "1  [LightGBM_BAG_L1_9, LightGBM_BAG_L1_11, LightG...   \n",
       "\n",
       "                               child_hyperparameters  \\\n",
       "0  {'num_boost_round': 20, 'num_threads': -1, 'le...   \n",
       "1                             {'ensemble_size': 100}   \n",
       "\n",
       "   child_hyperparameters_fit  \\\n",
       "0    {'num_boost_round': 11}   \n",
       "1       {'ensemble_size': 1}   \n",
       "\n",
       "                                   child_ag_args_fit          ancestors  \\\n",
       "0  {'max_memory_usage_ratio': 1.0, 'max_time_limi...                 []   \n",
       "1  {'max_memory_usage_ratio': 1.0, 'max_time_limi...  [LightGBM_BAG_L1]   \n",
       "\n",
       "             descendants  \n",
       "0  [WeightedEnsemble_L2]  \n",
       "1                     []  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(extra_info=True, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expanded leaderboard shows properties like how many features are used by each model (`num_features`), which other models are ancestors whose predictions are required inputs for each model (`ancestors`), and how much memory each model and all its ancestors would occupy if simultaneously persisted (`memory_size_w_ancestors`). See AutoGluon's leaderboard documentation for full details.\n",
    "\n",
    "To show scores for other metrics, you can specify the extra_metrics argument when passing in `test_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.369843</td>\n",
       "      <td>0.369843</td>\n",
       "      <td>0.255293</td>\n",
       "      <td>-7.175304</td>\n",
       "      <td>0.382837</td>\n",
       "      <td>1.166313</td>\n",
       "      <td>0.243411</td>\n",
       "      <td>18.828165</td>\n",
       "      <td>1.166313</td>\n",
       "      <td>0.243411</td>\n",
       "      <td>18.828165</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.369843</td>\n",
       "      <td>0.369843</td>\n",
       "      <td>0.255293</td>\n",
       "      <td>-7.175304</td>\n",
       "      <td>0.382837</td>\n",
       "      <td>1.170238</td>\n",
       "      <td>0.244729</td>\n",
       "      <td>18.831213</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  accuracy  balanced_accuracy  log_loss  \\\n",
       "0      LightGBM_BAG_L1    0.369843  0.369843           0.255293 -7.175304   \n",
       "1  WeightedEnsemble_L2    0.369843  0.369843           0.255293 -7.175304   \n",
       "\n",
       "   score_val  pred_time_test  pred_time_val   fit_time  \\\n",
       "0   0.382837        1.166313       0.243411  18.828165   \n",
       "1   0.382837        1.170238       0.244729  18.831213   \n",
       "\n",
       "   pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  \\\n",
       "0                 1.166313                0.243411          18.828165   \n",
       "1                 0.003925                0.001318           0.003048   \n",
       "\n",
       "   stack_level  can_infer  fit_order  \n",
       "0            1       True          1  \n",
       "1            2       True          2  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(\n",
    "    test_data, extra_metrics=[\"accuracy\", \"balanced_accuracy\", \"log_loss\"], silent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `log_loss` scores are negative. This is because metrics in AutoGluon are always shown in `higher_is_better` form. This means that metrics such as `log_loss` and `root_mean_squared_error` will have their signs __FLIPPED__, and values will be negative. This is necessary to avoid the user needing to know the metric to understand if higher is better when looking at leaderboard.\n",
    "\n",
    "One additional caveat: It is possible that `log_loss` values can be `-inf` when computed via `extra_metrics`. This is because the models were not optimized with `log_loss` in mind during training and may have prediction probabilities giving a class 0 (particularly common with K Nearest Neighbors models). Because `log_loss` gives infinite error when the correct class was given 0 probability, this results in a score of `-inf`. It is therefore recommended that `log_loss` not be used as a secondary metric to determine model quality. Either use `log_loss` as the `eval_metric` or avoid it altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting individual models\n",
    "Here’s how to specify a particular model to use for prediction instead of AutoGluon’s default model-choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from LightGBM_BAG_L1 model:  Other-service\n"
     ]
    }
   ],
   "source": [
    "# index of model to use\n",
    "i = 0\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred = predictor.predict(datapoint, model=model_to_use)\n",
    "print(f\"Prediction from {model_to_use} model: {model_pred.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily access information about the trained predictor or a particular model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = predictor.get_model_names()\n",
    "model_to_use = all_models[i]\n",
    "specific_model = predictor._trainer.load_model(model_to_use)\n",
    "\n",
    "# Objects defined below are dicts with information (not printed here as they are quite large):\n",
    "model_info = specific_model.get_info()\n",
    "predictor_information = predictor.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor also remembers which metric predictions should be evaluated with, which can be done with ground truth labels as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: accuracy on test data: 0.36984338212713685\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.36984338212713685,\n",
      "    \"balanced_accuracy\": 0.2552934550252014,\n",
      "    \"mcc\": 0.29567504858557747\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = predictor.predict_proba(test_data_nolabel)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the label columns remains in the `test_data` DataFrame, we can instead use the shorthand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: accuracy on test data: 0.36984338212713685\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.36984338212713685,\n",
      "    \"balanced_accuracy\": 0.2552934550252014,\n",
      "    \"mcc\": 0.29567504858557747\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "perf = predictor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Interpretability: Feature importance\n",
    "To better understand our trained predictor, we can estimate the overall importance of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 14 features using 1000 rows with 3 shuffle sets...\n",
      "\t24.92s\t= Expected runtime (8.31s per shuffle set)\n",
      "\t6.35s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>workclass</th>\n",
       "      <td>0.081333</td>\n",
       "      <td>0.010066</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>3</td>\n",
       "      <td>0.139015</td>\n",
       "      <td>0.023651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education-num</th>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.022517</td>\n",
       "      <td>0.012402</td>\n",
       "      <td>3</td>\n",
       "      <td>0.210023</td>\n",
       "      <td>-0.048023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>0.049333</td>\n",
       "      <td>0.016166</td>\n",
       "      <td>0.016989</td>\n",
       "      <td>3</td>\n",
       "      <td>0.141965</td>\n",
       "      <td>-0.043299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours-per-week</th>\n",
       "      <td>0.024333</td>\n",
       "      <td>0.006110</td>\n",
       "      <td>0.010188</td>\n",
       "      <td>3</td>\n",
       "      <td>0.059345</td>\n",
       "      <td>-0.010678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.019667</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>3</td>\n",
       "      <td>0.031595</td>\n",
       "      <td>0.007738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.059041</td>\n",
       "      <td>3</td>\n",
       "      <td>0.022172</td>\n",
       "      <td>-0.012839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-gain</th>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.264298</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018839</td>\n",
       "      <td>-0.016172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.028595</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>-0.001975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.211325</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010925</td>\n",
       "      <td>-0.008925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>native-country</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>-0.015160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital-status</th>\n",
       "      <td>-0.000333</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>-0.006950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-loss</th>\n",
       "      <td>-0.000667</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.788675</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>-0.007283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fnlwgt</th>\n",
       "      <td>-0.003333</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>0.770369</td>\n",
       "      <td>3</td>\n",
       "      <td>0.033058</td>\n",
       "      <td>-0.039724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                importance    stddev   p_value  n  p99_high   p99_low\n",
       "workclass         0.081333  0.010066  0.002534  3  0.139015  0.023651\n",
       "education-num     0.081000  0.022517  0.012402  3  0.210023 -0.048023\n",
       "sex               0.049333  0.016166  0.016989  3  0.141965 -0.043299\n",
       "hours-per-week    0.024333  0.006110  0.010188  3  0.059345 -0.010678\n",
       "age               0.019667  0.002082  0.001857  3  0.031595  0.007738\n",
       "education         0.004667  0.003055  0.059041  3  0.022172 -0.012839\n",
       "capital-gain      0.001333  0.003055  0.264298  3  0.018839 -0.016172\n",
       "race              0.001333  0.000577  0.028595  3  0.004642 -0.001975\n",
       "class             0.001000  0.001732  0.211325  3  0.010925 -0.008925\n",
       "native-country    0.000000  0.000000  0.500000  3  0.000000  0.000000\n",
       "relationship      0.000000  0.002646  0.500000  3  0.015160 -0.015160\n",
       "marital-status   -0.000333  0.001155  0.666667  3  0.006283 -0.006950\n",
       "capital-loss     -0.000667  0.001155  0.788675  3  0.005950 -0.007283\n",
       "fnlwgt           -0.003333  0.006351  0.770369  3  0.033058 -0.039724"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.feature_importance(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed via permutation-shuffling, these feature importance scores quantify the drop in predictive performance (of the already trained predictor) when one column’s values are randomly shuffled across rows. The top features in this list contribute most to AutoGluon’s accuracy (for predicting when/if a patient will be re-admitted to the hospital). Features with non-positive importance score hardly contribute to the predictor’s accuracy, or may even be actively harmful to include in the data (consider removing these features from your data and calling `fit` again). These scores facilitate interpretability of the predictor’s global behavior (which features it relies on for all predictions) rather than local explanations that only rationalize one particular prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Inference Speed: Model distillation\n",
    "\n",
    "While computationally-favorable, single individual models will usually have lower accuracy than weighted/stacked/bagged ensembles. Model Distillation offers one way to retain the computational benefits of a single model, while enjoying some of the accuracy-boost that comes with ensembling. The idea is to train the individual model (which we can call the student) to mimic the predictions of the full stack ensemble (the teacher). Like `refit_full()`, the `distill()` function will produce additional models we can opt to use for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training student models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distilling with teacher='WeightedEnsemble_L2', teacher_preds=soft, augment_method=spunge ...\n",
      "SPUNGE: Augmenting training data with 19990 synthetic samples for distillation...\n",
      "Distilling with each of these student models: ['LightGBM_DSTL', 'NeuralNetMXNet_DSTL', 'RandomForestMSE_DSTL', 'CatBoost_DSTL']\n",
      "Fitting model: LightGBM_DSTL ... Training model for up to 30.0s of the 30.0s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 161. Best iteration is:\n",
      "\t[161]\ttrain_set's soft_log_loss: -2.16912\tvalid_set's soft_log_loss: -1.8642\n",
      "\tNote: model has different eval_metric than default.\n",
      "\t-1.8642\t = Validation soft_log_loss score\n",
      "\t30.57s\t = Training runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Distilling with each of these student models: ['WeightedEnsemble_L2_DSTL']\n",
      "Fitting model: WeightedEnsemble_L2_DSTL ... Training model for up to 30.0s of the -1.21s of remaining time.\n",
      "\tNote: model has different eval_metric than default.\n",
      "\t-1.8642\t = Validation soft_log_loss score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Distilled model leaderboard:\n",
      "                      model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0             LightGBM_DSTL        0.4       0.087048  30.573700                0.087048          30.573700            1       True          3\n",
      "1  WeightedEnsemble_L2_DSTL        0.4       0.089478  30.578162                0.002430           0.004462            2       True          4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LightGBM_DSTL', 'WeightedEnsemble_L2_DSTL']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify much longer time limit in real applications\n",
    "student_models = predictor.distill(time_limit=30)\n",
    "student_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions from LightGBM_DSTL: [' Other-service', ' Farming-fishing', ' Exec-managerial', ' Other-service', ' Other-service']\n"
     ]
    }
   ],
   "source": [
    "preds_student = predictor.predict(test_data_nolabel, model=student_models[0])\n",
    "print(f\"predictions from {student_models[0]}: {list(preds_student)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_DSTL</td>\n",
       "      <td>0.371891</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.227564</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>30.573700</td>\n",
       "      <td>1.227564</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>30.573700</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2_DSTL</td>\n",
       "      <td>0.371891</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.229951</td>\n",
       "      <td>0.089478</td>\n",
       "      <td>30.578162</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.004462</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.369843</td>\n",
       "      <td>0.382837</td>\n",
       "      <td>1.079025</td>\n",
       "      <td>0.243411</td>\n",
       "      <td>18.828165</td>\n",
       "      <td>1.079025</td>\n",
       "      <td>0.243411</td>\n",
       "      <td>18.828165</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.369843</td>\n",
       "      <td>0.382837</td>\n",
       "      <td>1.084935</td>\n",
       "      <td>0.244729</td>\n",
       "      <td>18.831213</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  score_test  score_val  pred_time_test  \\\n",
       "0             LightGBM_DSTL    0.371891   0.400000        1.227564   \n",
       "1  WeightedEnsemble_L2_DSTL    0.371891   0.400000        1.229951   \n",
       "2           LightGBM_BAG_L1    0.369843   0.382837        1.079025   \n",
       "3       WeightedEnsemble_L2    0.369843   0.382837        1.084935   \n",
       "\n",
       "   pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0       0.087048  30.573700                 1.227564                0.087048   \n",
       "1       0.089478  30.578162                 0.002387                0.002430   \n",
       "2       0.243411  18.828165                 1.079025                0.243411   \n",
       "3       0.244729  18.831213                 0.005911                0.001318   \n",
       "\n",
       "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0          30.573700            1       True          3  \n",
       "1           0.004462            2       True          4  \n",
       "2          18.828165            1       True          1  \n",
       "3           0.003048            2       True          2  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presets\n",
    "\n",
    "If you know inference latency or memory will be an issue, then you can adjust the training process accordingly to ensure `fit()` does not produce unwieldy models.\n",
    "\n",
    "One option is to specify more lightweight presets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20210814_184406/\"\n",
      "Presets specified: ['good_quality_faster_inference_only_refit', 'optimize_for_deployment']\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20210814_184406/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    4500\n",
      "Train Data Columns: 14\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Sales', ' Adm-clerical', ' ?', ' Prof-specialty', ' Other-service', ' Machine-op-inspct', ' Craft-repair', ' Exec-managerial', ' Handlers-cleaners', ' Transport-moving']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 14 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9995555555555555\n",
      "Train Data Class Count: 14\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    13198.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.6 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])      : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.25 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 19.87s of the 29.81s of remaining time.\n",
      "\tRan out of time, stopping training early.\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 17.21s of the 27.15s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 2. Best iteration is:\n",
      "\t[2]\ttrain_set's multi_error: 0.645751\tvalid_set's multi_error: 0.666667\n",
      "\tTime limit exceeded... Skipping LightGBMXT_BAG_L1.\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 13.41s of the 23.35s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\ttrain_set's multi_error: 0.671443\tvalid_set's multi_error: 0.715556\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 10.65s of the 20.59s of remaining time.\n",
      "\t0.3706\t = Validation accuracy score\n",
      "\t1.37s\t = Training runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 8.96s of the 18.9s of remaining time.\n",
      "\t0.3566\t = Validation accuracy score\n",
      "\t2.56s\t = Training runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 6.08s of the 16.02s of remaining time.\n",
      "\tTime limit exceeded... Skipping CatBoost_BAG_L1.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.81s of the 9.27s of remaining time.\n",
      "\t0.3726\t = Validation accuracy score\n",
      "\t0.76s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 8.5s of the 8.49s of remaining time.\n",
      "\tRan out of time, stopping training early.\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L2.\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 7.59s of the 7.58s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\ttrain_set's multi_error: 0.66502\tvalid_set's multi_error: 0.702222\n",
      "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 5.47s of the 5.47s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\ttrain_set's multi_error: 0.594615\tvalid_set's multi_error: 0.697778\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 1.15s of the 1.14s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 42 due to low time. Expected time usage reduced from 7.9s -> 1.1s...\n",
      "\t0.3457\t = Validation accuracy score\n",
      "\t0.56s\t = Training runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 0.53s of the 0.52s of remaining time.\n",
      "\tWarning: Model is expected to require 22.9s to train, which exceeds the maximum time limit of 0.5s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L2.\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 0.19s of the 0.18s of remaining time.\n",
      "\tTime limit exceeded... Skipping CatBoost_BAG_L2.\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 0.14s of the 0.13s of remaining time.\n",
      "\tWarning: Model is expected to require 7.9s to train, which exceeds the maximum time limit of 0.1s, skipping model...\n",
      "\tTime limit exceeded... Skipping ExtraTreesGini_BAG_L2.\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 0.0s of the -0.0s of remaining time.\n",
      "\tWarning: Model is expected to require 7.9s to train, which exceeds the maximum time limit of 0.0s, skipping model...\n",
      "\tTime limit exceeded... Skipping ExtraTreesEntr_BAG_L2.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 29.81s of the -0.15s of remaining time.\n",
      "\t0.3457\t = Validation accuracy score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 30.17s ...\n",
      "Fitting model: RandomForestGini_BAG_L1_FULL ...\n",
      "\t0.3706\t = Validation accuracy score\n",
      "\t1.36s\t = Training runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1_FULL ...\n",
      "\t0.3566\t = Validation accuracy score\n",
      "\t2.57s\t = Training runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL ...\n",
      "\t0.3726\t = Validation accuracy score\n",
      "\t0.46s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Deleting model RandomForestGini_BAG_L1. All files under AutogluonModels/ag-20210814_184406/models/RandomForestGini_BAG_L1/ will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L1. All files under AutogluonModels/ag-20210814_184406/models/RandomForestEntr_BAG_L1/ will be removed.\n",
      "Deleting model WeightedEnsemble_L2. All files under AutogluonModels/ag-20210814_184406/models/WeightedEnsemble_L2/ will be removed.\n",
      "Deleting model RandomForestGini_BAG_L2. All files under AutogluonModels/ag-20210814_184406/models/RandomForestGini_BAG_L2/ will be removed.\n",
      "Deleting model WeightedEnsemble_L3. All files under AutogluonModels/ag-20210814_184406/models/WeightedEnsemble_L3/ will be removed.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20210814_184406/\")\n"
     ]
    }
   ],
   "source": [
    "presets = [\"good_quality_faster_inference_only_refit\", \"optimize_for_deployment\"]\n",
    "\n",
    "predictor_light = TabularPredictor(label=label, eval_metric=metric).fit(\n",
    "    train_data, presets=presets, time_limit=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight hyperparameters\n",
    "Another option is to specify more lightweight hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20210814_184441/\"\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20210814_184441/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    4500\n",
      "Train Data Columns: 14\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Sales', ' Adm-clerical', ' ?', ' Prof-specialty', ' Other-service', ' Machine-op-inspct', ' Craft-repair', ' Exec-managerial', ' Handlers-cleaners', ' Transport-moving']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 14 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9995555555555555\n",
      "Train Data Class Count: 14\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    12872.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.6 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])      : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.25 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.1111111111111111, Train Rows: 3998, Val Rows: 500\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 29.85s of the 29.85s of remaining time.\n",
      "No improvement since epoch 5: early stopping\n",
      "\t0.384\t = Validation accuracy score\n",
      "\t7.78s\t = Training runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 22.01s of the 22.01s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 9. Best iteration is:\n",
      "\t[8]\ttrain_set's multi_error: 0.518509\tvalid_set's multi_error: 0.634\n",
      "\t0.366\t = Validation accuracy score\n",
      "\t24.62s\t = Training runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.85s of the -2.77s of remaining time.\n",
      "\t0.39\t = Validation accuracy score\n",
      "\t0.17s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 32.97s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20210814_184441/\")\n"
     ]
    }
   ],
   "source": [
    "predictor_light = TabularPredictor(label=label, eval_metric=metric).fit(\n",
    "    train_data, hyperparameters=\"very_light\", time_limit=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can set hyperparameters to either `'light'`, `'very_light'`, or `'toy'` to obtain progressively smaller (but less accurate) models and predictors. Advanced users may instead try manually specifying particular models’ hyperparameters in order to make them faster/smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding models\n",
    "\n",
    "Finally, you may also exclude specific unwieldy models from being trained at all. Below we exclude models that tend to be slower (K Nearest Neighbors, Neural Network, models with custom larger-than-default hyperparameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20210814_184514/\"\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20210814_184514/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    4500\n",
      "Train Data Columns: 14\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Sales', ' Adm-clerical', ' ?', ' Prof-specialty', ' Other-service', ' Machine-op-inspct', ' Craft-repair', ' Exec-managerial', ' Handlers-cleaners', ' Transport-moving']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 14 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9995555555555555\n",
      "Train Data Class Count: 14\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    12883.81 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.6 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])      : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.25 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.1111111111111111, Train Rows: 3998, Val Rows: 500\n",
      "Excluded Model Types: ['KNN', 'NN', 'custom']\n",
      "\tFound 'NN' model in hyperparameters, but 'NN' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'KNN' model in hyperparameters, but 'KNN' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'KNN' model in hyperparameters, but 'KNN' is present in `excluded_model_types` and will be removed.\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 29.85s of the 29.85s of remaining time.\n",
      "No improvement since epoch 7: early stopping\n",
      "\t0.37\t = Validation accuracy score\n",
      "\t8.0s\t = Training runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 21.79s of the 21.78s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 25. Best iteration is:\n",
      "\t[13]\ttrain_set's multi_error: 0.588794\tvalid_set's multi_error: 0.628\n",
      "\t0.372\t = Validation accuracy score\n",
      "\t21.86s\t = Training runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.85s of the -0.24s of remaining time.\n",
      "\t0.39\t = Validation accuracy score\n",
      "\t0.17s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 30.43s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20210814_184514/\")\n"
     ]
    }
   ],
   "source": [
    "excluded_model_types = [\"KNN\", \"NN\", \"custom\"]\n",
    "predictor_light = TabularPredictor(label=label, eval_metric=metric).fit(\n",
    "    train_data, excluded_model_types=excluded_model_types, time_limit=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\".././images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
