{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"./images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "# MLU Day One Machine Learning - Hands On"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='orange'>Please make sure to run the below cell! It will allow you to print solutions for the code challenges.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coding libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Import utility functions that provide answers to challenges\n",
    "%load_ext autoreload\n",
    "%aimport dayone_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This hands-on notebook is meant to let you practice the concepts you have learned in this course so far.\n",
    "Here we explore a big database of books (books of different genres, from thousands of authors).<br/>\n",
    "\n",
    "We want to predict book prices using book features, such as genre, release data, ratings, number of reviews. \n",
    "This is a regression problem: we have a book price column in our dataset that we can use as labels.\n",
    "\n",
    "## Part I - Leaderboard Submission\n",
    "In the first part of the notebook you are going to learn how __AutoGluon__ can solve the book price prediction problem.<br/>\n",
    "\n",
    "You will learn how to build a simple and quick base model and then implement iterations of this model to make it better. To measure how well you are doing (and to see how the model improves) you have to submit your model's predictions to the [__MLU Leaderboard__](https://leaderboard.corp.amazon.com/tasks/718). Leaderboard will assess your performance against other participants and it also counts towards your course completion. \n",
    "\n",
    "We ask you to make 2 submissions in this section:<br/>\n",
    "1. First a simple prediction trained with a smaller dataset, in order to have your first submisison fast.\n",
    "2. Then another prediction trained with a full dataset, in order to submit an improved result.\n",
    "\n",
    "## Part II - Advanced AutoGluon\n",
    "In the second part of the notebook you will find some advanced features of AutoGluon to explore feature importance and explainability. You're welcome to use the insights you can gain from this section to make an additional 3rd submission. However, a quick word of warning - AutoGluon is very powerful in its base form so you might not see much additional model improvement.\n",
    "\n",
    "\n",
    "After the hands-on work, we will walk through the solution as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## How does this Hands-On notebook work?\n",
    "\n",
    "\n",
    "In this course, we are not trying to measure your coding skills, so you will find solutions throughout the notebook: \n",
    "All the challenges have answers that you can copy and paste into the challlenge coding area.\n",
    "\n",
    "**No matter how experienced and skilled you are with coding, you will be able to submit a solution!**\n",
    "\n",
    "Throughout the notebook, you will be presented with two kinds of exercises: __Knowledge Tasks__ and __Coding Challenges__. <br/>\n",
    "\n",
    "\n",
    "|Knowledge Tasks      | Coding Challenges |\n",
    "|:---    |   ---  |\n",
    "| No coding needed for theses tasks. <br /> Try to understand what is happening and run the cells & code associated to this. | These are challenges where you can practice your coding skills. <br /> Once done, uncomment the challenge asnwer and check your solution. <br />  __NOTE:__ Try hard to code your solution before looking at the answer. <br /> __Learn and Be Curious, right?__|\n",
    "| <img style=\"float: center;\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/>|<img style=\"float: center;\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/>| \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Part I - Leaderboard Submission\n",
    "Let's solve the book price prediction problem using __AutoGluon__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"5\">AutoGluon Installation</a>\n",
    "\n",
    "We need to begin by installing AutoGluon (documentation [here](https://auto.gluon.ai/stable/install.html)).  \n",
    "\n",
    "\n",
    "__NOTE__: This may take a few minutes to install (you can see that it has finished once the `[*]` symbol next to the cell disappears and turns into a number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install -qU pip\n",
    "!python3 -m pip install -qU setuptools wheel\n",
    "!python3 -m pip install -qU \"mxnet<2.0.0\"\n",
    "!python3 -m pip install -qU autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the libraries needed to work with our Tabular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the newly installed AutoGluon code library\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"5\">Business Problem Summary</a>\n",
    "\n",
    "Let's output an overview of our book price predicting __business problem__. <br/>\n",
    "\n",
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/>  \n",
    ">Run the function below for a description of the business problem and the data dictionary that will be used to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayone_utils.answer_html(\"BP1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 3. <a name=\"5\">Getting the Data</a>\n",
    "\n",
    "Let's get the data for our business problem.\n",
    "\n",
    ">  <img style=\"float: left; padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\" /> \n",
    ">  Run the cell below to load and take a look at the first samples of our train dataset. <br/>\n",
    "Compare it with your data dictionary to see if everything is there and if the data makes sense. This is a very basic check when performing __Data Exploration__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = TabularDataset(data=\"./datasets/training.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"5\">Training our model</a>\n",
    "\n",
    "We can train a model using AutoGluon with only a single line of code.  All we need to do is to tell it which column from the dataset we are trying to predict, and what the dataset is.\n",
    "\n",
    "For this first training, we are going to randomly sample 1000 samples of our train dataset in order to have a faster training.\n",
    "\n",
    "### Why are we splitting our data into train and validation below?\n",
    "The reason we split our original data into train and validation datasets is related to __overfitting__. Spliting a dataset to validate the performance is a useful way to identify if your model is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/>  Run the cell below to prepare the datasets (AutoGluon is doing all the magic for us). <br/>\n",
    "Here we are randomly selecting 1000 rows of our dataset and splitting it into train and validation datasets.\n",
    "> \n",
    "\n",
    "<br/>\n",
    "\n",
    "__NOTE__: The `random_state` parameter below alows to have repeatability when running the code multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below\n",
    "# Sampling 1000\n",
    "subsample_size = 1000  # subsample subset of data for faster demo, try setting this to much larger values\n",
    "df_train_smaller = df_train.sample(n=subsample_size, random_state=0)\n",
    "\n",
    "# Splitting in train and validation datasets\n",
    "train_data, val_data = train_test_split(\n",
    "    df_train, test_size=0.1, shuffle=True, random_state=23\n",
    ")\n",
    "train_data_smaller, val_data_smaller = train_test_split(\n",
    "    df_train_smaller, test_size=0.1, shuffle=True, random_state=23\n",
    ")\n",
    "\n",
    "# Printing the first rows\n",
    "train_data_smaller.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model with a small sample\n",
    "\n",
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/> \n",
    "For this first training we are going to use the smaller dataset with 1000 samples of our original train dataset in order to have a faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below\n",
    "smaller_predictor = TabularPredictor(label=\"Price\").fit(train_data=df_train_smaller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Training Output\n",
    "AutoGluon outputs a lot of information about what is happening.\n",
    "\n",
    "<img style=\"float: left;\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "<br/><br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "> After the prediction above finishes, examine the output and try to find the information below in the print out messages from AutoGluon. <br/>\n",
    "1. What is the shape of your training dataset?\n",
    "2. What kind of ML problem type does AutoGluon infer (classification, regression, ...)? Remember, you've never mentioned what kind of problem type it is; you only provided the label column.\n",
    "3. What does AutoGluon suggest in case it inferred the wrong problem type?\n",
    "4. Identify the kind of data preprocessing and feature engineering performed by AutoGluon.\n",
    "5. Find the basic statistics about your label in the print statements from AutoGluon.\n",
    "6. How many extra features were generated besides the originals in our dataset? What was the runtime for that?\n",
    "7. What is the evaluation metric used?\n",
    "8. What does AutoGluon suggests to do if it inferred the wrong metric?\n",
    "9. How much of the training data was used for validation when splitted?\n",
    "10. Identify the folder where the models are saved.\n",
    "11. Identify where AutoGluon saved your prediction.\n",
    "12. Enter a specific model folder and take a quick look to see the file format.\n",
    "\n",
    "__Please, try hard to identify all information above before uncommenting the answer below. <br/>\n",
    "Day One is about Learn and Be Curious, right?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################# LIST YOUR ANSWERS HERE #################\n",
    "1. <br/>\n",
    "2. <br/>\n",
    "3. <br/>\n",
    "4. <br/>\n",
    "5. <br/>\n",
    "6. <br/>\n",
    "7. <br/>\n",
    "8. <br/>\n",
    "9. <br/>\n",
    "10. <br/>\n",
    "11. <br/>\n",
    "12. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_FIT_INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">AutoGluon Leaderboard</a>\n",
    "Now let's take a look at all the information AutoGluon provides via its __leaderboard function__. <br/> \n",
    "\n",
    "__NOTE__: Don't confuse this with the MLU Leaderboard. The MLU Leaderboard is where you will make submissions with the predictions from your trained models; the AutoGluon leaderboard function is a summary of all models that AutoGluon trained.\n",
    "\n",
    "<br/>\n",
    "\n",
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "> Run the cell below and take a closer look at AutoGluon's leaderboard output. <br/>\n",
    "__Which one is the best model?__\n",
    "\n",
    "<br/>\n",
    "\n",
    "__NOTE__: As AutoGluon only maximizes metrics, you will see a negative RMSE value, for prioritization purposes only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below\n",
    "smaller_predictor.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_BEST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a name=\"5\">Making a Prediction</a>\n",
    "### Now that your model is trained, let's use it to predict Prices\n",
    "\n",
    "We are now reading the test dataset that was not used to train our model. It is a good practice to assess if your model is __overfitting__. \n",
    "#### Why are we using a different dataset that was not used so far during the training step?\n",
    "We should always run a final model performance assessment using data that was unseen by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/> \n",
    "Run the cell below to load the test dataset that we will use for the MLU leaderboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below\n",
    "df_test_leaderboard = TabularDataset(\"./datasets/mlu-leaderboard-test.csv\")\n",
    "\n",
    "# We show the first row there.\n",
    "df_test_leaderboard.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "Use this new dataset as input to the model you have just trained to predict Book Prices on it <br/>\n",
    "__TIP:__ look at the AutoGluon Tasks documentation and look for function __predict__ to see how to implement it [here](https://auto.gluon.ai/api/autogluon.task.html#autogluon.tabular.TabularPredictor.predict).\n",
    "\n",
    "__Please, try hard to identify all information above before uncomment the answer below. You know, it is about Learn and Be Curious, right?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "############## END OF CODE ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_PRED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <a name=\"5\">Your First MLU Leaderboard Submission</a>\n",
    "### Now you are ready for your first submission to our MLU Leaderboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/> \n",
    "> Run the cell below to save your prediction file in the format expected by the MLU Leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pandas columns\n",
    "df_submission = pd.DataFrame(columns=[\"ID\", \"Price\"])\n",
    "# Creating ID column from ID list\n",
    "df_submission[\"ID\"] = df_test_leaderboard[\"ID\"].tolist()\n",
    "# Creating label column from price prediction list\n",
    "df_submission[\"Price\"] = price_prediction\n",
    "# saving your csv file for Leaderboard submission\n",
    "df_submission.to_csv(\n",
    "    \"./datasets/predictions/Prediction_to_Leaderboard.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do a quick check to see if the file is ok related to the IDs expected\n",
    "> <img style=\"float: left; padding-right: 30px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/> \n",
    "> 1. Run the cell below to check if your submission file has the right IDs for the MLU Leaderboard.\n",
    "> 2. If the difference is zero you are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below\n",
    "print(\"Double-check submission file against the original test file\")\n",
    "sample_submission_df = pd.read_csv(\"./datasets/mlu-leaderboard-test.csv\", sep=\",\")\n",
    "print(\n",
    "    \"Differences between project result IDs and sample submission IDs:\",\n",
    "    (sample_submission_df[\"ID\"] != df_submission[\"ID\"]).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Prediction File and Submitting\n",
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\" align=\"left\"/> \n",
    "> 1. Download the file you just saved to your local machine. <br/>\n",
    "> 2. Follow the instructions on the Leaderboard submission page [here](https://leaderboard.corp.amazon.com/tasks/718/submit) to submit your file.\n",
    "\n",
    "<br>\n",
    "You can find your submission file in the folder <code>datasets > predictions</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. <a name=\"5\">Your Second MLU Leaderboard Submission with the Full Train Dataset</a>\n",
    "\n",
    "<img style=\"float: left;\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "<br/><br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "> Now that you made your first submission using the small sample from your dataset, repeat the process using the full dataset and submit again to see if your score gets better.<br>\n",
    "If you don't know how to write the code for this, uncomment the challenge answer; copy and paste it in the section below.\n",
    "\n",
    "__NOTE__: It should take around 12-15 minutes to run this training with our CPU. Just in case, use the `time_limit` parameter (in seconds) to limit the run time to 20 minutes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "############## END OF CODE ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_FULL_PRED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second MLU Leaderboard Submission with the Full Train Dataset\n",
    "\n",
    "><img style=\"float: left; padding-right: 20px\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "1. Run the AutoGluon leaderboard function for this and the smaller dataset into the first cell below.\n",
    "2. Run the leaderboard function again for the full dataset into the second cell below.\n",
    "3. Compare the performances.\n",
    "\n",
    "__How can you explain the differences in `score_val` and `fit_time` columns?__\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## FIRST CODE HERE ####################\n",
    "\n",
    "\n",
    "############## END OF CODE ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## SECOND CODE HERE ###############\n",
    "\n",
    "\n",
    "############## END OF CODE ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_FULL_LEAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the second submission for MLU Leaderboard ready</a>\n",
    "\n",
    "><img style=\"float: left; padding-right: 20px\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "> Write the code that creates the output file using the predictions from your second model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "############## END OF CODE ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_FULL_SUBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do a quick check to see if the file is ok related to the IDs expected\n",
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/> \n",
    "1. Run the cell below to check if your submission file has the right IDs for the MLU Leaderboard.\n",
    "2. If the difference is zero you are good to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below\n",
    "print(\"Double-check submission file against the original test file\")\n",
    "sample_submission_df = pd.read_csv(\"./datasets/mlu-leaderboard-test.csv\", sep=\",\")\n",
    "print(\n",
    "    \"Differences between project result IDs and sample submission IDs:\",\n",
    "    (sample_submission_df[\"ID\"] != df_full_submission[\"ID\"]).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/> \n",
    "> Submit again to MLU leaderboard to improve your score. For the submission use the link as before [here](https://leaderboard.corp.amazon.com/tasks/718/submit).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Part II - Advanced AutoGluon\n",
    "## 9. <a name=\"5\">AutoGluon Advanced Features</a>\n",
    "\n",
    "Now that you have made your first Leaderboard submission, let's practice using some advanced features of AutoGluon. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. <a name=\"4\">Explainability</a>\n",
    "\n",
    "There are growing business needs and legislative regulations that require explanations of why a model made a certain decision.<br/>\n",
    "To better understand our trained predictor, we can estimate the overall importance of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.1 <a name=\"5\">Feature Importance</a> \n",
    "A feature’s importance score represents the performance drop that results when the model makes predictions on a perturbed copy of the dataset where this feature’s values have been randomly shuffled across rows. A feature score of 0.01 would indicate that the predictive performance dropped by 0.01 when the feature was randomly shuffled. The higher the score a feature has, the more important it is to the model’s performance. If a feature has a negative score, this means that the feature is likely harmful to the final model, and a model trained without that feature  would be expected to achieve a better predictive performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img style=\"float: left;padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\" align=\"left\"/> \n",
    "> Run the code below to see the output of the AutoGluon feature importance function for the first model we have run, with only 1000 samples. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the code below\n",
    "smaller_predictor.feature_importance(val_data_smaller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.2. <a name=\"5\">An Experiment on Tuning the Data</a>\n",
    "\n",
    "With AutoGluon you don't have to worry about which model to chose; indeed you can focus on the data itself. \n",
    "In the book price case, there are a few columns which are clearly very poorly encoded, most importantly the ```Edition``` column. <br/>\n",
    "For this experiment, let's use our small dataset __df_train_smaller__ to make everything run a bit faster.\n",
    "\n",
    "> <img style=\"float: left;padding-right: 20px\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "> Use the functions below to clean things up a bit and expand that data out.<br/>\n",
    "For this experiment, our feature engineering taks will be:<br/><br/>\n",
    ">1. Splitting the Column ```Edition``` into three new ones: ```hard_paper```, ```year``` and ```month```\n",
    ">2. Creating two numerical features based on the features ```Reviews``` and ```Ratings```, named ```Reviews-n``` and ```Ratings-n``` respectively.\n",
    ">3. Drop the old columns from the dataset: ```Edition```,  ```Reviews``` and ```Ratings```. \n",
    "\n",
    "__Please, try hard to solve the challenge before uncommenting for the answer below.__ <br/>\n",
    "\n",
    "\n",
    "__Day One is about Learn and Be Curious, right?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def first_num(in_val):\n",
    "    num_string = in_val.split(\" \")[0]\n",
    "    digits = re.sub(r\"[^0-9\\.]\", \"\", num_string)\n",
    "    return float(digits)\n",
    "\n",
    "\n",
    "def year_get(in_val):\n",
    "    m = re.compile(r\"\\d{4}\").findall(in_val)\n",
    "    # print(in_val, m)\n",
    "    if len(m) > 0:\n",
    "        return int(m[0])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def month_get(in_val):\n",
    "    m = re.compile(r\"Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec\").findall(in_val)\n",
    "    # print(in_val, m)\n",
    "    if len(m) > 0:\n",
    "        return m[0]\n",
    "    else:\n",
    "        return \"None\"\n",
    "\n",
    "\n",
    "def drop_features(in_feat):\n",
    "    train_data_feateng.drop(in_feat, axis=1, inplace=True)\n",
    "    val_data_feateng.drop(in_feat, axis=1, inplace=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "############## END OF CODE ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_FEAT_ENG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><img style=\"float: left;padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/> \n",
    ">Now print the dataset with the new features to see how they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the code below\n",
    "train_data_feateng.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of Data Preprocessing: Identifying Missing values\n",
    "By doing the feature engineering above we introduced a new challenge. \n",
    "We might now have some missing data.\n",
    "\n",
    "> <img style=\"float: left;padding-right: 20px\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "> Try to identify the features that may have missing values and how many are missing. <br/>\n",
    "__Are there any missing values?__\n",
    "\n",
    "__Please, try hard to solve the challenge before uncommenting for the answer below.__ <br/>\n",
    "\n",
    "\n",
    "__Day One is about Learn and Be Curious, right?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "############## END OF CODE ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_MISSING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "> Let's train the model again with these new and clean features to compare results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "############## END OF CODE ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_PRED_FEAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "> Try to identify the features that may have missing values and how many are missing. <br/>\n",
    "__Are there any significant differences?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## FIRST CODE FROM THE ANSWER HERE ####################\n",
    "\n",
    "\n",
    "############## END OF CODE ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## SECOND CODE FROM THE ANSWER HERE ####################\n",
    "\n",
    "\n",
    "############## END OF CODE #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_LEAD_COMP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img style=\"float: left; padding-right: 30px\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "1. Run the AutoGluon `feature_importance` function for original smaller dataset into the first cell below.\n",
    "2. Run the feature_importance function again for the feature engineered dataset into the second cell below.\n",
    "3. Compare the results.\n",
    "\n",
    "__Are there any significant differences?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE FOR THE ORIGINAL DATASET FEATURE IMPORTANCE HERE ####################\n",
    "\n",
    "\n",
    "############## END OF CODE ############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE FOR THE FEATURE ENGINEERED DATASET FEATURE IMPORTANCE HERE  ####################\n",
    "\n",
    "\n",
    "############## END OF CODE #########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_FEAT_COMP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. <a name=\"5\">Further Enhancement</a>\n",
    "So far we have worked with AutoGluon's default settings; however there are settings that let you tune things further.  When you have text, the best default line to run is the following.  Letting this run (for 14 hours instead of < 20 minutes) will produce a model that comes in second on the global leaderboard, all with only about an hour of human labor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/> \n",
    "> Now it is time to train your model using using AutoGluon __enhanced version__\n",
    "\n",
    "\n",
    "<br/>\n",
    "For this experiment we will use a time limit of 20 min (`time_limit` in seconds below).\n",
    "\n",
    "__NOTE__: 20 minutes may not be enough to have a better score than your previous submission. If you have time, try running for more than 20 minutes to improve your performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we're working on CPU based instances, we need to tell AutoGluon to train without GPU\n",
    "import os\n",
    "os.environ['AUTOGLUON_TEXT_TRAIN_WITHOUT_GPU']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_predictor = TabularPredictor(label=\"Price\").fit(\n",
    "    train_data=df_train, time_limit=20 * 60, hyperparameters=\"multimodal\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to make Your Final Submission to the MLU Leaderboard</a>\n",
    "\n",
    "> <img style=\"float: left;padding-right: 20px\" src=\"./images/challenge_robot.png\" alt=\"drawing\" width=\"130\"/> \n",
    "> Now make a final prediction and submit this to MLU leaderboard.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "############## END OF CODE ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHALLENGE ANSWER\n",
    "# dayone_utils.answer_html(\"CH_FINAL_SUBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do a quick check to see if the file is ok related to the IDs expected\n",
    "><img style=\"float: left; padding-right: 30px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/> \n",
    "> 1. Run the cell below to check if your submission file has the right IDs for the MLU Leaderboard.\n",
    "2. If the difference is zero you are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below\n",
    "print(\"Double-check submission file against the original test file\")\n",
    "sample_submission_df = pd.read_csv(\"./datasets/mlu-leaderboard-test.csv\", sep=\",\")\n",
    "print(\n",
    "    \"Differences between project result IDs and sample submission IDs:\",\n",
    "    (sample_submission_df[\"ID\"] != df_enhanced_submission[\"ID\"]).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"./images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "    \n",
    "## Congrats for Finishing this Hands On!!\n",
    "In the next module, __Code Walkthrough and Advanced AutoGluon__ we are going do a walkthrough over your solutions and also show a notebook that implements an __end-to-end__ solution, deploying your model for use in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. <a name=\"5\">Before You Go</a>\n",
    "> <img style=\"float: left; padding-right: 20px\" src=\"./images/task_robot.png\" alt=\"drawing\" width=\"100\"/> \n",
    ">After you are done with this Hands On, you can clean all model artifacts uncommenting and executing the cell below.<br/>\n",
    "\n",
    "__It's always a good practice to clean up everything when you are done.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r AutogluonModels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
